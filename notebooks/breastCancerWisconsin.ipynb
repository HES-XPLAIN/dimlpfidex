{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6032ef9f-92e2-44dc-a11b-b224342bee29",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Exploring Dimlp and Fidex rule generation for breast cancer classification\n",
    "\n",
    "**Introduction:**\n",
    "\n",
    "By the end of this notebook, you'll have a solid understanding of how to use Dimlp to train the model and the Fidex algorithms to extract rules.\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "    1. Understand the importance of interpretability in medical problems.\n",
    "    2. Introduce Dimlp and Fidex as powerful XAI techniques.\n",
    "    3. Understand how to use Dimlp and Fidex.\n",
    "    4. Showcase the capabilities of HES-Xplain in implementing Dimlp and Fidex algorithms.\n",
    "    5. Provide practical insights into applying Dimlp and Fidex to breast cancer classifiers through an interactive notebook.\n",
    "    6. Foster a community of XAI enthusiasts and practitioners.\n",
    "\n",
    "**Outline:**\n",
    "\n",
    "    1. Dataset and Problem Statement.\n",
    "    2. Load the dataset.\n",
    "    3. Model training.\n",
    "    4. Local rules generation - Fidex.\n",
    "    5. Global ruleSet generation - FidexGlo.\n",
    "    6. Conclusion.\n",
    "    7. References."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f882f8-9ee5-44a3-9091-bc5cd14c896e",
   "metadata": {},
   "source": [
    "# Dataset and Problem Statement\n",
    "\n",
    "The Breast Cancer Dataset used in this use case consists of 569 data samples representing patients who have tumors. Each patient has 30 attributes, computed out of an image, used to determine if the tumor is malignant (first class) or benign (second class). This dataset is available on [Kaggle](https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data) and can be imported with sklearn. It is composed of 455 training samples and 114 testing samples. \n",
    "\n",
    "The 30 attributes are composed of 10 real-valued features computed for each cell nucleus :\n",
    "\n",
    "- **Radius**: Mean of distances from the center to points on the perimeter.\n",
    "- **Texture**: Standard deviation of gray-scale values.\n",
    "- **Perimeter**\n",
    "- **Area**\n",
    "- **Smoothness**: Local variation in radius lengths.\n",
    "- **Compactness**: $\\frac{\\text{perimeter}^2}{\\text{area}} - 1.0$\n",
    "- **Concavity**: Severity of concave portions of the contour.\n",
    "- **Concave Points**: Number of concave portions of the contour.\n",
    "- **Symmetry**\n",
    "- **Fractal Dimension**: \"Coastline approximation\" - 1.\n",
    "\n",
    "For each tumor image, the mean, standard error, and worst errors were computed and appear as different attributes.\n",
    "\n",
    "**Problem Statement:** Our objective is to build a robust classifier capable of accurately classifying breast cancer samples among the 2 classes. By leveraging deep learning techniques and Fidex algorithms, we aim to not only achieve high classification performance but also gain insights into the attributes that contribute to the classification decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfedb29-a2b0-4a64-9958-da3f1463a78d",
   "metadata": {},
   "source": [
    "# Load the dataset\n",
    "\n",
    "We'll start by importing all libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2697a5-c92e-40a6-86ce-018b897baea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.utils import shuffle\n",
    "from dimlpfidex.dimlp import dimlpBT\n",
    "from IPython.display import Image, display\n",
    "from trainings.normalization import normalization\n",
    "from trainings.computeRocCurve import computeRocCurve\n",
    "from dimlpfidex.fidex import fidex, fidexGloRules, fidexGloStats, fidexGlo\n",
    "\n",
    "class Unbuffered(object):\n",
    "   def __init__(self, stream):\n",
    "       self.stream = stream\n",
    "   def write(self, data):\n",
    "       self.stream.write(data)\n",
    "       self.stream.flush()\n",
    "   def writelines(self, datas):\n",
    "       self.stream.writelines(datas)\n",
    "       self.stream.flush()\n",
    "   def __getattr__(self, attr):\n",
    "       return getattr(self.stream, attr)\n",
    "\n",
    "\n",
    "sys.stdout = Unbuffered(sys.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dade571b-4876-45b5-9d09-a76791165a75",
   "metadata": {},
   "source": [
    "We download the dataset with sklearn, then shuffle it randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7b7b29-d07d-4750-820f-2270937d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datas, classes = shuffle(load_breast_cancer().data, load_breast_cancer().target)\n",
    "\n",
    "dataset, classes = load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "classesLabels = list(load_breast_cancer().target_names)\n",
    "\n",
    "# classes one-hot encoding \n",
    "classes = pd.get_dummies(classes, dtype=\"uint8\")\n",
    "\n",
    "# rename classes\n",
    "classes = classes.rename(\n",
    "    columns={\n",
    "        classes.columns[0]: classesLabels[0],\n",
    "        classes.columns[1]: classesLabels[1],\n",
    "    }\n",
    ")\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d87dd5-2233-43e2-9f95-777a1f7bda99",
   "metadata": {},
   "source": [
    "We separate the dataset into a training set (75%) and test set (25%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0ed78b-b077-4136-84a6-9f176f33c223",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbSamples = dataset.shape[0]\n",
    "nbAttributes = len(dataset.columns)\n",
    "nbClasses = len(classes.columns)\n",
    "\n",
    "cutoff = int(0.75 * nbSamples)\n",
    "\n",
    "trainData = dataset.iloc[:cutoff, :]\n",
    "testData = dataset.iloc[cutoff:, :]\n",
    "trainClasses = classes.iloc[:cutoff]\n",
    "testClasses = classes.iloc[cutoff:]\n",
    "\n",
    "assert (trainData.shape[0] + testData.shape[0]) == nbSamples\n",
    "assert (trainClasses.shape[0] + testClasses.shape[0]) == nbSamples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d123e58b-b85d-4b43-9e1d-fcc11b65eaa3",
   "metadata": {},
   "source": [
    "We display the number of training and testing data and check that there is the same number of data as classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce94b0e-e7ab-4d8f-9082-cb57da8b17cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {trainData.shape[0]} train datas and {trainClasses.shape[0]} train classes\")\n",
    "print(f\"There are {testData.shape[0]} test datas and {testClasses.shape[0]} test classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7724b51a-a76f-4e3e-9e27-b1b0a40a148d",
   "metadata": {},
   "source": [
    "Now, we save the all the data in text files in a specific format into BCWDataset directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a104b59-41da-4963-b799-70cb8544f8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        print(f\"Folder '{folder_path}' created successfully.\")\n",
    "\n",
    "rootDir = \"data/BCWDataset/\"\n",
    "create_folder(rootDir)\n",
    "\n",
    "# defining filenames\n",
    "datasetFile = \"datas.txt\"\n",
    "trainDataFile = \"trainData.txt\"\n",
    "testDataFile = \"testData.txt\"\n",
    "trainClassFile = \"trainClass.txt\"\n",
    "testClassFile = \"testClass.txt\"\n",
    "\n",
    "dataset.to_csv(rootDir+datasetFile, header=False, index=False)\n",
    "3\n",
    "testData.to_csv(rootDir+testDataFile, header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a05c5c-ad03-4e58-96d7-6bbe04f79a0c",
   "metadata": {},
   "source": [
    "Finally, we normalize the data so that the model learns better. We save the mean and std of each attribute in the file normalization_stats.txt. We use the normalization file located in the trainings module. To display the parameters, you can just call normalization with no parameters, or with -h or --help. Without further details, we normalize with this command :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350d6c19-c2af-4a8e-893f-4b86fa9460e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalization(\"--help\")\n",
    "\n",
    "args = f\"\"\"\n",
    "        --data_files [{trainDataFile},{testDataFile}] \n",
    "        --nb_attributes {nbAttributes} \n",
    "        --missing_values NaN \n",
    "        --root_folder {rootDir}\n",
    "        \"\"\"\n",
    "normalization(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b34fc4-1e80-4eab-84ec-a0a4a28626fb",
   "metadata": {},
   "source": [
    "To show attribute names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6634d478-316f-464d-9199-a8ea39883b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = list(dataset.columns)\n",
    "print(attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2ed3d0-cb53-4371-9ec8-76500422c93d",
   "metadata": {},
   "source": [
    "And class names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5ee674-09b8-47a2-a927-d2b3de0c3c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = list(classes.columns)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb8d44f-be2c-485a-99dc-5e90ea7e74a1",
   "metadata": {},
   "source": [
    "We store attribute and class names in a file named `attributes.txt` to identify features within rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba892ed9-0ae5-4711-a998-07bc668764f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributesFileName = \"attributes.txt\" \n",
    "attributesFileContent = attributes + classesLabels\n",
    "attributesFileContent = [x.replace(\" \", \"_\") for x in attributesFileContent]\n",
    "\n",
    "with open(rootDir+attributesFileName, \"w\") as f:\n",
    "    for item in attributesFileContent:\n",
    "        f.write(item+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c01997-d95c-4c99-8ea4-3dfa31d7ba2f",
   "metadata": {},
   "source": [
    "# Model training\n",
    "Here is an example of how to train a DimlpBT model with our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25db56b3",
   "metadata": {},
   "source": [
    "First, let's display all the DimlpBT parameters available:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69eb22f",
   "metadata": {},
   "source": [
    "To configure our training, we will use this configuration file, located at `data/BCWTemplates/config_dimlpBT.json`:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"root_folder\": \"data/BCWDataset\",\n",
    "    \"train_data_file\": \"trainData_normalized.txt\",\n",
    "    \"train_class_file\": \"trainClass.txt\",\n",
    "    \"test_data_file\": \"testData_normalized.txt\",\n",
    "    \"test_class_file\": \"testClass.txt\",\n",
    "    \"attributes_file\": \"attributes.txt\",\n",
    "    \"train_pred_outfile\": \"predTrain.out\",\n",
    "    \"test_pred_outfile\": \"predTest.out\",\n",
    "    \"console_file\": \"resultDimlpBT.txt\",\n",
    "    \"with_rule_extraction\": true,\n",
    "    \"global_rules_outfile\": \"dimlpBTRules.rls\",\n",
    "    \"weights_outfile\": \"weights.wts\",\n",
    "    \"stats_file\": \"stats.txt\",\n",
    "    \"nb_attributes\": 30,\n",
    "    \"nb_classes\": 2,\n",
    "    \"hidden_layers\": [5]\n",
    "}\n",
    "```\n",
    "\n",
    "> A detailed list of parameters is available [here](https://hes-xplain.github.io/documentation/algorithms/dimlp/dimlpBT/). If you want to create your own configuration file, we recommend using our [configuration file creator](https://hes-xplain.github.io/documentation/gui/)\n",
    "\n",
    "\n",
    "\n",
    "We can start training our model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e804d87b-b2dc-4946-9e99-caf1fa3fa066",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = dimlpBT(\"--json_config_file data/BCWTemplates/config_dimlpBT.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c12044-2b09-4b7c-82b4-e6285ef55dfc",
   "metadata": {},
   "source": [
    "You can check all outputs inside the `data/BCWDataset` folder.\n",
    "\n",
    "Inside `stats.txt` you can see the train and test accuracy as well as the sum squared errors. \n",
    "\n",
    "There are also the train predictions, test predictions, the weights, and the dimlp rules files.\n",
    "\n",
    "In the file `dimlpBTRules.txt`, you can see the rule set, some statistics on the rules, and some statistics on the ruleset. Each rule is composed of antecedents and target class as well as the number of covering of the rule, which is the number of examples that verify (or \"activate\") the rule (even if the class is not correct).\n",
    "\n",
    "In the `Training set` and `Testing set` sections, each rule is composed, left to right, of:\n",
    "\n",
    "The number of covered samples, the number of correct covered samples, the number of false covered samples, and the accuracy of the rule.\n",
    "\n",
    "Between the `Training set` and `Testing set` sections, There are statistics of the ruleset:\n",
    "\n",
    "- The number of rules in the set.\n",
    "- The mean total and the mean number of antecedents per rule.\n",
    "- The covering of the rule.\n",
    "- The rules' accuracy.\n",
    "- The fidelity to the model is the percentage of covered samples that are correct concerning the model's decision.\n",
    "- The model accuracy if we keep only the samples for which the model and the rules decision agree.\n",
    "- The default rule activation rate, which is the percentage of samples for which no rule is activated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0865c6d-1648-4a61-8962-e81690b8d491",
   "metadata": {},
   "source": [
    "# Local rules generation - Fidex\n",
    "\n",
    "Fidex is an algorithm used in classification problems that allows us to obtain a rule explaining the decision class of the model for a given test sample. It enables us to grasp the model's decision-making process and to better understand the importance of each parameter in discerning the nature of the tumor, distinguishing between benign and malignant cases.\n",
    "\n",
    "Now we can generate some local rules to explain the models' results. We can start with launching Fidex on one test sample. This will generate a rule explaining the sample locally. It is local because the algorithm searches a rule only for one sample.\n",
    "Fidex is located in the fidex module. Let's take a look at the parameters :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368222c7-c0f8-4af0-a6a7-7adf290b7192",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = fidex(\"--help\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4603d85-d0eb-4fb1-bd50-e66cac47b936",
   "metadata": {},
   "source": [
    "We see that we need the number of attributes and classes, the train and test data and class files, as well as train and test predictions, weights file name, and rule output file name where the rules will be saved.<br>\n",
    "We specify as well the saved folder and the attributes.<br>\n",
    "We also need to specify the normalization file obtained from training, to denormalize the values in the generated rule, otherwise the values will be normalized and impossible to interpret.<br>\n",
    "\n",
    "To see what happens, we launch it with just one sample, and we save beforehand the test data sample in a file with its class and predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add3ff77-f54f-4288-b7fa-3c780a617653",
   "metadata": {},
   "outputs": [],
   "source": [
    "testPredsFile = \"predTest.out\"\n",
    "testSampleFile = \"testDataSample.txt\"\n",
    "testPreds = pd.read_csv(rootDir+testPredsFile, sep=\" \", header=None, index_col=None).iloc[:, :nbClasses]\n",
    "testDatasNormalized = pd.read_csv(rootDir+testDataFile, header=None, index_col=None)\n",
    "\n",
    "sampleSelected = 0\n",
    "assert(sampleSelected < nbSamples)\n",
    "\n",
    "# extract a sample to generate local rule\n",
    "sampleData = testData.iloc[sampleSelected, :nbAttributes].to_list()\n",
    "samplePred = testPreds.iloc[sampleSelected, :].to_list()\n",
    "sampleClasses = testData.iloc[sampleSelected, nbAttributes:].to_list()\n",
    "\n",
    "# write the sample, classes and predictions in the testSampleFile file (file writing format must be respected)\n",
    "with open(rootDir+testSampleFile, 'w') as f:\n",
    "    f.write(\" \".join(str(x) for x in sampleData) + '\\n')\n",
    "    f.write(\" \".join(str(x) for x in samplePred) + '\\n')\n",
    "    f.write(\" \".join(str(x) for x in sampleClasses) + '\\n')\n",
    "\n",
    "#res = fidex.fidex(\"--train_data_file trainData_normalized.txt --train_pred_file predTrain.out --train_class_file trainClass.txt --test_data_file testDataSample.txt --weights_file Weights/weights.wts --attributes_file attributes.txt --rules_outfile rule.rls --nb_attributes 30 --nb_classes 2 --normalization_file normalization_stats.txt --root_folder data/BCWDataset\")\n",
    "res = fidex(\"--json_config_file data/BCWTemplates/config_fidexOne.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dcf258-9284-4a41-817b-9158f321a2df",
   "metadata": {},
   "source": [
    "You can see the walkthrough of the algorithm and the rule extracted. The rule is also saved in the rule.rls file. With the rule, we see also the covering size of the rule on the training set, the fidelity, the accuracy, and the confidence of the rule. The confidence shows how much the rule is confident with his choices, with respect to the prediction values.<br>\n",
    "\n",
    "Now, we execute Fidex with all test samples. We send the console output in the fidexResult.txt file and save the global statistics in fidexStats.txt. **It should take about a minute**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2475dc1f-0815-476b-a695-9869598dcd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#res = fidex.fidex(\"--train_data_file trainData_normalized.txt --train_pred_file predTrain.out --train_class_file trainClass.txt --test_data_file testData_normalized.txt --test_class_file testClass.txt --test_pred_file predTest.out --weights_file Weights/weights.wts --attributes_file attributes.txt --rules_outfile fidexRules.rls --stats_file fidexStats.txt --console_file fidexResult.txt --nb_attributes 30 --nb_classes 2 --normalization_file normalization_stats.txt --root_folder data/BCWDataset\")\n",
    "res = fidex(\"--json_config_file data/BCWTemplates/config_fidex.json\")\n",
    "\n",
    "if (res == 0):\n",
    "    print(\"Fidex done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c4f8fe-ec3d-4231-866a-e4361dd323be",
   "metadata": {},
   "source": [
    "You can see the rules generated for each sample in the file fidexRules.rls. The global statistics on the test set appear in statsFidex.txt. There is the mean covering size per rule, the mean number of antecedents per rule, and the mean rule fidelity, accuracy, and confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2c1cad-7802-47e5-bc09-aa28967913cc",
   "metadata": {},
   "source": [
    "# Global ruleSet generation - FidexGlo\n",
    "We've seen how to compute a rule that explains the decision of the model for a specific sample. Now, we will generalize a ruleset that characterizes the whole train dataset. That means that for each training sample, there is a rule in the set of rules that explains the model's decision for this sample. We will use this global ruleset to explain the results obtained on new test samples. If there is a rule of the ruleset corresponding to the sample, we take this one and get a global explanation for the sample. If there is none, we call Fidex and only have a local explanation.<br>\n",
    "\n",
    "To get the ruleSet we execute fidexGloRules which is located in the fidex module. Here are the possible parameters :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8286ec5a-306b-4048-84cb-7358c2647863",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = fidexGloRules(\"--help\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665be28c-c788-41c2-ae00-b7a7b3118c2b",
   "metadata": {},
   "source": [
    "We use nearly the same parameters as for Fidex but we only need train data. We need to choose a heuristic for fidexGloRules, we choose the optimal to get better results. We don't forget to add the normalization file.<br>\n",
    "**It should take about 3 minutes**. If you have several processors available, you should add the parameter nb_threads with the number of processors that you want to use, it can speed up the process a lot. If you want to accelerate the process even more, you can use some dropout, the algorithm will randomly skip some dimensions or some hyperplans. For example, you can put: -d 0.5 -h 0.5, to skip half dimensions and half hyperplans, which should be about 4 times faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e0d897-ab14-4e8d-a0f9-da7fe35bfb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#res = fidex.fidexGloRules(\"--train_data_file trainData_normalized.txt --train_pred_file predTrain.out --train_class_file trainClass.txt --weights_file Weights/weights.wts --attributes_file attributes.txt --nb_attributes 30 --nb_classes 2 --heuristic 1 --global_rules_outfile globalRules.rls --console_file fidexGloRulesResult.txt --normalization_file normalization_stats.txt --root_folder data/BCWDataset\")\n",
    "res = fidexGloRules(\"--json_config_file data/BCWTemplates/config_fidexGloRules.json\")\n",
    "if (res == 0):\n",
    "    print(\"FidexGloRules done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e7b780-5fc6-4d0a-a82e-1b86ed6853d4",
   "metadata": {},
   "source": [
    "You can see the console result in the file fidexGloRulesResult.txt and the ruleset in the file globalRules.rls.\n",
    "The algorithm is random, so each execution may generate different rules and a different number of them. It should generate about 25 rules. You can see at the top of the globalRules file the number of rules, the mean covering number per rule, and the mean number of antecedents. Here is an example of a rule that you may obtain:<br>\n",
    "\n",
    "Rule 1: WORST_CONCAVE_POINTS<0.111377 AREA_ERROR<37.691316 -> BENIGN <br>\n",
    "   Train Covering size : 217 <br>\n",
    "   Train Fidelity : 1 <br>\n",
    "   Train Accuracy : 0.990783 <br>\n",
    "   Train Confidence : 0.984811 <br>\n",
    "\n",
    "This rule is the first rule, which means that it's the rule with the maximum covering. Here, 217 train samples verify this rule. She is 100% fidel with the model and has about 99% train accuracy.\n",
    "This rule says that if you have worst concave points lower than 0.111377 and an area error lower than 37.691316, then your breast cancer is benign. And this rule is accurate, on train test, at 99% and has 98% of confidence.<br>\n",
    "\n",
    "Now, we can see some global statistics on the test set, and also some statistics directly on each rule. So we will see the test accuracy on this rule.<br>\n",
    "We execute fidexGloStats which is located in the fidex module. First, let's check the parameters :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6aaa70-4ffa-4205-99f8-840d67b260cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = fidexGloStats(\"--help\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b7ebed-01b9-4016-addb-d99839f7be74",
   "metadata": {},
   "source": [
    "We need the test files(data, prediction, class), the rules file, and the number of attributes and classes. We need to specify the attribute file as well. We choose to save the results in the file fidexGloStats.txt. With --global_rules_outfile we can generate the statistics on rules which will modify the rules file. If you want to keep the first ruleSet unchanged, you should give another name. Finally, with --positive_class_index we can specify the positive class in order to get a ROC curve and some other statistics on false and true positives/negatives. We will design the malignant class as positive. So here is the command :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375ef5c0-aca8-4e1a-a842-ed229d23e8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#res = fidex.fidexGloStats(\"--test_data_file testData.txt --test_pred_file predTest.out --test_class_file testClass.txt --attributes_file attributes.txt --global_rules_file globalRules.rls --global_rules_outfile globalRules.rls --nb_attributes 30 --nb_classes 2 --stats_file fidexGloStats.txt --positive_class_index 0 --root_folder data/BCWDataset\")\n",
    "res = fidexGloStats(\"--json_config_file data/BCWTemplates/config_fidexGloStats.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c523c382-b2ed-413e-84e7-c0d39c6d1bf5",
   "metadata": {},
   "source": [
    "Here you can see the global statistics on the test set. As the previous calculations have randomness, each execution is a bit different. So here we will discuss the same case as before, so you may have slightly different results. In our case, we had about 98% fidelity, which is good, and a rule accuracy(96.5%) about 1.5% lower than the model accuracy(98%). So the rules seem to classify a bit worse. The explainability rate is the percentage of samples for which we can find a rule in the rules set. For the others, we need to execute Fidex (this is the default rule rate). In our case, we had more than a 97% explainability rate, so only in 3% of cases do we need to compute Fidex. Each rule can activate many rules. Here on average, a sample activates 5 correct rules and 0.05 wrong rules. A wrong rule is a rule with which the model doesn't agree. For example, if the rule says malign and the model says benign. Something interesting is the model test accuracy when rules and model agree. You can see that, generally, the accuracy increases if we consider samples where rules and model agree, and increases even more if we take only the activated rules (when there are no activated rules, we choose the model prediction). That means that the rules confirm well the model decision, but when no rule is found, the model decision may as well be wrong. <br>\n",
    "\n",
    "Finally, we have the statistics on the positive/negative decisions of the model and of the rules. For the model's decisions, we had 1% false positives and 7% false negatives. That means that when the model says that it's malign, it has a 99% chance of being malign. But when it says that it's benign, there is a 7% risk that it is malign in reality. So the precision is 97% and the recall is 93%.\n",
    "\n",
    "In the case of the rules decisions, the rules decision is the same as the model if we find a correct rule. If there is no rule, we can launch Fidex, so it's also the same decision as the model because it will create a fidel rule. The only scenario when the decision changes from the model's is when some rules are activated, all rules decide the same class which is not the class chosen by the model. The results are really close with 95% of precision and 93% of recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5400680-5058-4f75-97f1-552b1cead6d8",
   "metadata": {},
   "source": [
    "In globalRules file, you can now see the statistics of rules on the test set. Here is the same rule as before that I have now:<br>\n",
    "\n",
    "Rule 1: MEAN_CONCAVE_POINTS<0.041282 WORST_CONCAVE_POINTS<0.150829 -> BENIGN <br>\n",
    "   Train Covering size : 217 --- Test Covering size : 54 <br>\n",
    "   Train Fidelity : 1 --- Test Fidelity : 0.981481 <br>\n",
    "   Train Accuracy : 0.990783 --- Test Accuracy : 1 <br>\n",
    "   Train Confidence : 0.984811 --- Test Confidence : 0.979816 <br>\n",
    "\n",
    "You can see that the rule no longer always agrees with the model, only in 98.15% of cases. However, the rule accuracy has increased with the test. That means that the rule is very good in reality, with 100% of correct classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c457745a-6030-486e-95d4-33040fe430ea",
   "metadata": {},
   "source": [
    "We can get a ROC curve obtained with the test set. Here are the parameters of computeRocCurve which is located in the trainings package :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338b8a62-832e-45f7-8540-9ec4a7928447",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = computeRocCurve(\"--help\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ec3ff5-a6db-4478-9d36-87f2ddbc3247",
   "metadata": {},
   "source": [
    "We execute computeRocCurve :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c817149-bae3-4ce2-8bc8-074a27747705",
   "metadata": {},
   "outputs": [],
   "source": [
    "#res = computeRocCurve('--test_class_file testClass.txt --test_pred_file predTest.out --positive_class_index 0 --output_roc outRoc.png --stats_file stats.txt --root_folder data/BCWDataset --nb_classes 2 --show_params false')\n",
    "res = computeRocCurve(\"--json_config_file data/BCWTemplates/config_roc.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a27b096-fbfd-4c67-a303-58a5b404169b",
   "metadata": {},
   "source": [
    "The AUC score is added to the stats file and the ROC curve is saved in the outRoc.png file. We get an AUC of about 99%. <br>\n",
    "We can visualize the AUC curve :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd579681-492d-4900-84ae-69aeef690a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename=\"data/BCWDataset/outRoc.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6709efc3-5ac9-41d8-927e-1c5fedddef82",
   "metadata": {},
   "source": [
    "Finally, it's possible to launch fidexGlo on a test set. It will, for each test sample, search in the global ruleset for all the rules that verify the sample's properties and match the decision of the model. If no rule verifies the properties, it calls Fidex on the test sample. In the other case, it gives all the correct and wrong activated rules.<br>\n",
    "\n",
    "FidexGlo is located in the fidex module. Here are the possible parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ccff77-b9a8-4c97-b3f7-2f8756935bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = fidexGlo(\"--help\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c845e1fe-9226-4aa7-874d-eb582695afda",
   "metadata": {},
   "source": [
    "As we want to use Fidex on test sample not covered by the global ruleset, we need to specify the Fidex parameters. We don't forget to add the normalization file and launch it with :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec3cd7f-502b-49c4-b1ce-28f219b8d5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#res = fidex.fidexGlo(\"--test_data_file testData.txt --test_pred_file predTest.out --global_rules_file globalRules.rls --nb_attributes 30 --nb_classes 2 --attributes_file attributes.txt --explanation_file explanations.txt --console_file fidexGloResults.txt --root_folder data/BCWDataset --with_fidex true --train_data_file trainData.txt --train_pred_file predTrain.out --train_class_file trainClass.txt --test_class_file testClass.txt --weights_file Weights/weights.wts --normalization_file normalization_stats.txt\")\n",
    "res = fidexGlo(\"--json_config_file data/BCWTemplates/config_fidexGlo.json\")\n",
    "\n",
    "if res == 0:\n",
    "    print(\"Explanations generated\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccf58f4-b70b-4586-ad3c-345d9a5d0823",
   "metadata": {},
   "source": [
    "The explanations for each test sample can then be found in the file explanations.txt. We do not find a global rule for around 2.6% of the samples, for which we calculate a local rule using Fidex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7f3208-c538-40eb-8c76-31e75d687198",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Throughout this notebook, we've learned how to use DimlpBT to train a dataset and get some explaining rules, and we got into the different Fidex algorithms to get some local and global explanations, and some statistics on the model's decisions. We used the example of the breast cancer dataset from Wisconsin and we saw that the algorithms perform very well. It gives some good explanations of the model's decision and does it very fast, especially if a fidexGlo ruleset has been computed beforehand. You can try with any tabular dataset you want, you just have to remember to transform the data in a good format. <be>\n",
    "\n",
    "We didn't speak about every single parameter, you can try to change every hyperparameter you want to see how it goes. An interesting parameter that we didn't consider is the parameter for decision threshold (--decision_threshold) in Fidex algorithms. It allows us to change prediction with respect to a specific threshold on the positive class. If the model gives a score prediction, for the positive class, greater than this threshold, the model prediction is considered to be the positive class, even if another class obtains a higher score. If the recall is not good enough, it's possible to improve it this way. Another parameter is the seed. It allows you to remove the randomness of the algorithms and get the same result in each execution for the same parameters and data.<br>\n",
    "\n",
    "We considered only a simple tabular dataset. It's also possible to use it for an image classification problem. To see how we can train a dataset with convolutions, we recommend exploring the Mnist notebook for a hands-on experience **Insérer lien notebook Mnist**. There are also other training methods, like an MLP or decision trees. The Keel Australian notebook goes through these and shows what to do when we have categorical attributes **Insérer Keel Australian notebook**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605dcb45-4862-4268-bef2-068cf96de3d9",
   "metadata": {},
   "source": [
    "# Références\n",
    "\n",
    "Article DimlpBT de Guido\n",
    "Article Fidex de Guido (à venir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
