{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "471d58e0-9f96-4164-9fb3-eeec8952f796",
   "metadata": {},
   "source": [
    "# Exploring CNN models and Fidex rule generation for Cracks classification\n",
    "\n",
    "**Introduction:**\n",
    "\n",
    "Welcome to HES-Xplain, our interactive platform designed to facilitate explainable artificial intelligence (XAI) techniques. In this use case, we dive into the classification with CNN models trained on the Cracks dataset. By the end of this notebook, you'll have a solid understanding of how to use a CNN to train the model and the Fidex algorithms to extract rules.\n",
    "\n",
    "This notebook is an alternative to the [`Exploring CNN models and Fidex rule generation for MNIST classification`](TODO). It achieves the same goal but with a different dataset to show another application and result of our algorithms.\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "    1. Observe a different use case where XAI can be used.\n",
    "    3. Understand how to use CNNs and Fidex.\n",
    "    4. Showcase the versatility of HES-Xplain using a different dataset and training model.\n",
    "    5. Provide practical insights into applying CNNs and Fidex to Cracks classifiers through an interactive notebook.\n",
    "    6. Foster a community of XAI enthusiasts and practitioners.\n",
    "\n",
    "**Outline:**\n",
    "\n",
    "    1. Dataset and Problem Statement.\n",
    "    2. Model training.\n",
    "    3. Local rules generation - Fidex.\n",
    "    4. Global ruleSet generation - FidexGlo.\n",
    "    5. Explanation and image generation.\n",
    "    6. Conclusion.\n",
    "    7. References."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675fe6ce-7c47-4227-aeba-2bac55bde9af",
   "metadata": {},
   "source": [
    "# Workspace Setup\n",
    "\n",
    "\n",
    "This section download the required dataset from our GitHub and huggingface.co repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e48b4b5f-a07b-4abc-a321-d566e1e99718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeanmarc/dimlpfidex/notebooks/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset successfully extracted to: data/CracksDataset\n"
     ]
    }
   ],
   "source": [
    "# download and extract dataset\n",
    "import zipfile\n",
    "import os\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "REPO_ID = \"HES-XPLAIN/Cracks\"\n",
    "FILENAME = \"Cracks.zip\"\n",
    "dataset_file_path = hf_hub_download(repo_id=REPO_ID, filename=FILENAME, repo_type=\"dataset\")\n",
    "extract_path = 'data/CracksDataset'\n",
    "\n",
    "# Unzip the file\n",
    "with zipfile.ZipFile(dataset_file_path, 'r') as zip_ref:\n",
    "    # Extract all the files\n",
    "    for member in zip_ref.namelist():\n",
    "        # Remove the folder name 'Cracks' from the path\n",
    "        member_path = os.path.relpath(member, start='Cracks')\n",
    "        if member_path == '.':\n",
    "            continue\n",
    "        # Create the appropriate path in the destination directory\n",
    "        target_path = os.path.join(extract_path, member_path)\n",
    "        # Create any directories needed to house the file\n",
    "        os.makedirs(os.path.dirname(target_path), exist_ok=True)\n",
    "        # Write the file to the directory\n",
    "        with zip_ref.open(member) as source, open(target_path, 'wb') as target:\n",
    "            target.write(source.read())\n",
    "\n",
    "print(f\"Dataset successfully extracted to: {extract_path}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62a8f3b4-f169-4ec4-bf69-3f325ae269ec",
   "metadata": {},
   "source": [
    "# Dataset and Problem Statement\n",
    "\n",
    "The dataset we'll be working with is called Cracks and is available on [Kaggle](https://www.kaggle.com/datasets/arunrk7/surface-crack-detection). It consists of 40'000 data samples representing images of concrete surfaces containing or not some cracks. Each image sample has a shape of 227x227x3 representing a RGB pixel value ranging from 0 to 255.\n",
    "\n",
    "**Problem Statement:** Our objective is to build a robust CNN classifier capable of accurately classifying the images among those that contain cracks(class 1) and those that don't(class 0). By leveraging deep learning techniques and Fidex algorithms, we aim to not only achieve high classification performance but also gain insights into the attributes (pixels here) that contribute to the classification decisions.\n",
    "\n",
    "We'll start by importing all libraries. A warning might appear, but there's no need to worry about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c2697a5-c92e-40a6-86ce-018b897baea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeanmarc/dimlpfidex/notebooks/.venv/lib/python3.10/site-packages/numpy/core/getlimits.py:542: UserWarning: Signature b'\\x00\\xd0\\xcc\\xcc\\xcc\\xcc\\xcc\\xcc\\xfb\\xbf\\x00\\x00\\x00\\x00\\x00\\x00' for <class 'numpy.longdouble'> does not match any known type: falling back to type probe function.\n",
      "This warnings indicates broken support for the dtype!\n",
      "  machar = _get_machar(dtype)\n"
     ]
    }
   ],
   "source": [
    "import os, sys, re\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from dimlpfidex.fidex import fidex, fidexGloRules, fidexGloStats, fidexGlo\n",
    "from trainings.cnnTrn import cnnTrn\n",
    "\n",
    "# utility function to preview a file entirely or only the first `nlines` lines\n",
    "def previewFile(filepath, nlines=-1):\n",
    "    lines = \"\"\n",
    "\n",
    "    with open(filepath, \"r\") as f:\n",
    "        if nlines == -1:\n",
    "            for line in f:\n",
    "                lines += line\n",
    "        else:\n",
    "            for _ in range(nlines):\n",
    "                try:\n",
    "                    lines += next(f)\n",
    "                except StopIteration:\n",
    "                    break\n",
    "    print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0eec7b-28cd-4b00-9f51-a2f5a4fe403c",
   "metadata": {},
   "source": [
    "We already preprocessed the data and saved it in the `data/CracksDataset` folder. We kept only 20% of all images to shorten the process and from these images, we kept 20% of the images for the test, so we have 6'400 training samples and 1'600 testing samples. Since the images are too large for our algorithms and processing them would take too much time, we resized them to 64x64x1 in grayscale format. It's not necessary to normalize the data as it is done during the CNN training process. In the upcoming chapter, we'll use our prepared dataset to train our CNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c01997-d95c-4c99-8ea4-3dfa31d7ba2f",
   "metadata": {},
   "source": [
    "# Model training\n",
    "\n",
    "It's time now to train our model. We will use a special type of model called a CNN (convolutional neural network).\n",
    "\n",
    "We use our Python program called [cnnTrn](https://github.com/HES-XPLAIN/dimlpfidex/blob/main/trainings/cnnTrn.py). Let's begin with printing the program help message to observe every option available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2566d658-96d1-4675-b76f-7d6d70730ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: \n",
      "--train_data_file <str> --test_data_file <str> --original_input_size <pair<int [1, inf[>> --nb_channels <int [1,inf[> --model <{small, large, vgg, resnet}> --data_format <{normalized_01, classic, other}> --nb_classes <int [1,inf[> [-h, --help] [--json_config_file <str>] [--root_folder <str>] [--train_class_file <str>] [--test_class_file <str>] [--train_valid_pred_outfile <str>] [--test_pred_outfile <str>] [--valid_ratio <float ]0,inf[>] [--valid_data_file <str>] [--valid_class_file <str>] [--weights_outfile <str>] [--stats_file <str>] [--console_file <str>] [--nb_epochs <int [1,inf[>] [--nb_quant_levels <int [3,inf[>] [--K <float ]0,inf[>] [--model_input_size <pair<int [1, inf[>>] [--seed <{int [0,inf[}>]\n",
      "\n",
      "This is a parser for cnnTrn\n",
      "\n",
      "\n",
      "Parameters:\n",
      "\n",
      "  ---------------------------------------------------------------------\n",
      "\n",
      "  Warning! The files are located with respect to the root folder dimlpfidex.\n",
      "\n",
      "  The arguments can be specified in the command or in a json configuration file with --json_config_file your_config_file.json.\n",
      "\n",
      "  ----------------------------\n",
      "\n",
      "  Required parameters:\n",
      "\n",
      "  --train_data_file <str>                          Path to the file containing the train portion of the dataset\n",
      "  --test_data_file <str>                           Path to the file containing the test portion of the dataset\n",
      "  --original_input_size <pair<int [1, inf[>>       Original input size size\n",
      "  --nb_channels <int [1,inf[>                      Number of channels in the input (3 for RGB image, 1 for B&W image)\n",
      "  --model <{small, large, vgg, resnet}>            Training model\n",
      "  --data_format <{normalized_01, classic, other}>  Format of the values of the data, normalized_01 if the data are normalized between 0 and 1, classic\n",
      "                                                   if they are between 0 and 255\n",
      "  --nb_classes <int [1,inf[>                       Number of classes in the dataset\n",
      "\n",
      "  ----------------------------\n",
      "\n",
      "  Optional parameters:\n",
      "\n",
      "  -h --help                                        show this help message and exit\n",
      "  --json_config_file <str>                         Path to the JSON file that configures all parameters. If used, this must be the sole argument and\n",
      "                                                   must specify the file's relative path\n",
      "  --root_folder <str>                              Path to the folder, based on main default folder dimlpfidex, containing all used files and where\n",
      "                                                   generated files will be saved. If a file name is specified with another option, its path will be\n",
      "                                                   relative to this root folder> (default: \"\")\n",
      "  --train_class_file <str>                         Path to the file containing the train true classes of the dataset, mandatory if classes are not\n",
      "                                                   specified in train_data_file\n",
      "  --test_class_file <str>                          Path to the file containing the test true classes of the dataset, mandatory if classes are not\n",
      "                                                   specified in test_data_file\n",
      "  --train_valid_pred_outfile <str>                 Path to the file where the output train and validation (in this order) prediction will be stored\n",
      "                                                   (default: predTrain.out)\n",
      "  --test_pred_outfile <str>                        Path to the file where the test predictions will be stored (default: predTest.out)\n",
      "  --valid_ratio <float ]0,inf[>                    Percentage of train data taken for validation (default: 0.1)\n",
      "  --valid_data_file <str>                          Path to the file containing the validation portion of the dataset\n",
      "  --valid_class_file <str>                         Path to the file containing the validation true classes of the dataset, mandatory if classes are\n",
      "                                                   not specified in valid_data_file. BE CAREFUL if there is validation files, and you want to use\n",
      "                                                   Fidex algorithms later, you will have to use both train and validation datas given here in the\n",
      "                                                   train datas and classes of Fidex\n",
      "  --weights_outfile <str>                          Path to the file where the output trained weights of the model will be stored (default:\n",
      "                                                   weights.wts)\n",
      "  --stats_file <str>                               Path to the file where the train and test accuracy will be stored (default: stats.txt)\n",
      "  --console_file <str>                             Path to the file where the terminal output will be redirected. If not specified, all output will be\n",
      "                                                   shown on your terminal\n",
      "  --nb_epochs <int [1,inf[>                        Number of model training epochs (default: 80)\n",
      "  --nb_quant_levels <int [3,inf[>                  Number of stairs in the staircase activation function (default: 50)\n",
      "  --K <float ]0,inf[>                              Parameter to improve dynamics by normalizing input data (default: 1.0)\n",
      "\n",
      "  ----------------------------\n",
      "\n",
      "  CNN parameters (optional):\n",
      "\n",
      "  --model_input_size <pair<int [1, inf[>>          Input size in the model. A small size is recommended to speed up the process. The size is modified\n",
      "                                                   if necessary\n",
      "  --seed <{int [0,inf[}>                           Seed for random number generation, 0=random. Anything else than 0 is an arbitrary seed that can be\n",
      "                                                   reused to obtain the same randomly generated sequence and therefore getting same results (default:\n",
      "                                                   0)\n",
      "\n",
      "  ----------------------------\n",
      "\n",
      "Execution example :\n",
      "\n",
      "cnnTrn('--model small --train_data_file trainData.txt --train_class_file trainClass.txt --test_data_file testData.txt --test_class_file testClass.txt --valid_data_file validData.txt --valid_class_file validClass.txt --original_input_size (28,28) --nb_channels 1 --data_format classic --nb_classes 10 --root_folder dimlp/datafiles/Mnist')\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "status = cnnTrn(\"--help\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69eb22f",
   "metadata": {},
   "source": [
    "The output reveals various options. Among these, we'll focus on the required parameters (and the `--root_folder` for convenience). Since we've already generated the train and test data files, our next step is determining the data's shape and the model we want to train. As we saw, an image has now a shape of 64x64x1 and has values between 0 and 255, which correspond to the classic format. There are 2 possible classes, with or without cracks, and 64*64=4096 attributes.\n",
    "\n",
    "With these parameters in place, we can proceed to run our CNN model, allowing the remaining options to be determined by their default settings.\n",
    "\n",
    "First, let's define some commonly used parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a4d9fa3-ea11-44bd-9e13-6d9a37113316",
   "metadata": {},
   "outputs": [],
   "source": [
    "rootDir = \"data/CracksDataset/\"\n",
    "trainDataFile = \"trainData.txt\"\n",
    "trainClassFile = \"trainClass.txt\"\n",
    "testDataFile = \"testData.txt\"\n",
    "testClassFile = \"testClass.txt\"\n",
    "\n",
    "nclasses = 2\n",
    "nattributes = 4096\n",
    "\n",
    "trainPredFile = \"predTrain.out\"\n",
    "testPredFile = \"predTest.out\"\n",
    "weightsFile = \"weights.wts\"\n",
    "\n",
    "globalRulesFile = \"globalRules.rls\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33594c60-282d-430d-8242-685ca85cba50",
   "metadata": {},
   "source": [
    "As it may take some time, the training has already been done with the VGG16 model. If you wish to train it yourself, you can uncomment and run the next instructions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e804d87b-b2dc-4946-9e99-caf1fa3fa066",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = f\"\"\"\n",
    "        --root_folder {rootDir}\n",
    "        --train_data_file {trainDataFile}\n",
    "        --train_class_file {trainClassFile}\n",
    "        --test_data_file {testDataFile}\n",
    "        --test_class_file {testClassFile}\n",
    "        --original_input_size [64,64]\n",
    "        --data_format classic\n",
    "        --nb_channels 1\n",
    "        --nb_classes {nclasses}\n",
    "        --model vgg\n",
    "        --nb_epochs 1\n",
    "        \"\"\"\n",
    "\n",
    "#status = cnnTrn(args)\n",
    "#if (status == 0):\n",
    "#    print(\"cnnTrn done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c12044-2b09-4b7c-82b4-e6285ef55dfc",
   "metadata": {},
   "source": [
    "The algorithm generated the train and test predictions as well as the model's weights and statistics. All outputs are saved inside the `data/CracksDataset` folder.\n",
    "\n",
    "The train and test accuracy are stored in the `stats.txt` file. Let's visualize these accuracies :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4779c4ed-14fb-43dd-ba37-bcdc1bd76851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy : 99.930555%.\n",
      "Testing accuracy : 99.624997%.\n"
     ]
    }
   ],
   "source": [
    "previewFile(\"data/CracksDataset/stats.txt\", 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0865c6d-1648-4a61-8962-e81690b8d491",
   "metadata": {},
   "source": [
    "# Local rules generation - Fidex\n",
    "\n",
    "Now we can generate some local rules to explain the models' results. We can start with launching [Fidex](https://hes-xplain.github.io/documentation/algorithms/fidex/fidex/) on one test sample. This will generate a rule explaining the sample locally. It is local because the algorithm searches a rule only for one sample.\n",
    "\n",
    "First of all, let's take a look at Fidex's arguments :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "368222c7-c0f8-4af0-a6a7-7adf290b7192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Warning! The files are located with respect to the root folder dimlpfidex.\n",
      "The arguments can be specified in the command or in a json configuration file with --json_config_file your_config_file.json.\n",
      "\n",
      "----------------------------\n",
      "\n",
      "Required parameters:\n",
      "\n",
      "--train_data_file <str>       Path to the file containing the train portion of the dataset\n",
      "--train_class_file <str>      Path to the file containing the train true classes of the dataset, not mandatory if classes are specified in train data file\n",
      "--train_pred_file <str>       Path to the file containing predictions on the train portion of the dataset\n",
      "--test_data_file <str>        Path to the file containing the test sample(s) data, prediction (if no --test_pred_file) and true class(if no --test_class_file)\n",
      "--weights_file <str>          Path to the file containing the trained weights of the model (not mandatory if a rules file is given with --rules_file)\n",
      "--rules_file <str>            Path to the file containing the trained rules to be converted to hyperlocus (not mandatory if a weights file is given with --weights_file)\n",
      "--rules_outfile <str>         Path to the file where the output rule(s) will be stored. If a .json extension is given, rules are saved in JSON format\n",
      "--nb_attributes <int [1,inf[> Number of attributes in the dataset\n",
      "--nb_classes <int [2,inf[>    Number of classes in the dataset\n",
      "\n",
      "----------------------------\n",
      "\n",
      "Optional parameters: \n",
      "\n",
      "--json_config_file <str>      Path to the JSON file that configures all parameters. If used, this must be the sole argument and must specify the file's relative path\n",
      "--root_folder <str>           Path to the folder, based on main default folder dimlpfidex, containing all used files and where generated files will be saved. If a file name is specified with another option, its path will be relative to this root folder\n",
      "--test_class_file <str>       Path to the file containing the test true classes of the dataset. If at least --test_pred_file is specified, --test_data_file needs to have only test datas and eventually classes on same line (don't add --test_class_file in this case)\n",
      "--test_pred_file <str>        Path to the file containing predictions on the test portion of the dataset\n",
      "--attributes_file <str>       Path to the file containing the labels of attributes and classes\n",
      "--stats_file <str>            Path to the file where statistics concerning the algorithm execution will be stored\n",
      "--console_file <str>          Path to the file where the terminal output will be redirected. If not specified, all output will be shown on your terminal\n",
      "--max_iterations <int [1,inf[>\n",
      "                              Maximum iteration number, also the maximum possible number of attributes in a rule, it should be 25 when working with images (default: 10)\n",
      "--min_covering <int [1,inf[>  Minimum number of samples covered by the generated rules (default: 2)\n",
      "--covering_strategy <bool>    Whether to use the covering strategy : if no rule is found with min_covering, find best rule with best covering using dichotomic search. Decreases min_fidelity if needed (default: True)\n",
      "--max_failed_attempts <int [0,inf[>\n",
      "                              Maximum number of failed attempts to find a Fidex rule when the covering is 1 and the covering strategy is used (default: 30)\n",
      "--min_fidelity <float [0,1]>  Minimal rule fidelity accepted when generating a rule (default: 1.0)\n",
      "--lowest_min_fidelity <float [0,1]>\n",
      "                              Minimal min_fidelity to which we agree to go down during the covering_strategy (default: 0.75)\n",
      "--dropout_dim <float [0,1]>   Probability of dropping a dimension during rule extraction (default: 0.0)\n",
      "--dropout_hyp <float [0,1]>   Probability of dropping a hyperplane during rule extraction (default: 0.0)\n",
      "--decision_threshold <float [0,1]>\n",
      "                              The decision threshold used for predictions, you need to specify the index of the positive class if you want to use it\n",
      "--positive_class_index <int [0,nb_classes-1]>\n",
      "                              Index of the positive class for the usage of a decision threshold, index starts at 0\n",
      "--nb_quant_levels <int [3,inf[>\n",
      "                              Number of stairs in the staircase activation function (default: 50)\n",
      "--normalization_file <str>    Path to the file containing the mean and standard deviation of some attributes. Used to denormalize the rules if specified\n",
      "--mus <list<float ]-inf,inf[>>\n",
      "                              Mean or median of each attribute index to be denormalized in the rules\n",
      "--sigmas <list<float ]-inf,inf[>>\n",
      "                              Standard deviation of each attribute index to be denormalized in the rules\n",
      "--normalization_indices <list<int [0,nb_attributes-1]>>\n",
      "                              Attribute indices to be denormalized in the rules, only used when no normalization_file is given, index starts at 0 (default: [0,...,nb_attributes-1])\n",
      "--seed <int [0,inf[>          Seed for random num"
     ]
    }
   ],
   "source": [
    "status = fidex(\"--help\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7db4c60-c4c0-4056-8e64-0eb737e77e7f",
   "metadata": {},
   "source": [
    "Let's have a closer look at the Fidex help output. We can observe that there are **required parameters**. Let's have a look at them:\n",
    "\n",
    "- `--train_data_file`: a file containing features from the training portion of the dataset\n",
    "- `--train_pred_file`: a file containing predictions from the training portion of the dataset\n",
    "- `--train_class_file`: a file containing classes from the training portion of the dataset\n",
    "- `--test_data_file`: a file containing samples to be used when generating a local rule\n",
    "- `--weights_file`: a file containing weights from a model training \n",
    "- `--rules_file`: a file containing the rules generated by a model training (in our case, we don't need it because we already have a `weights file` from the CNN training)\n",
    "- `--rules_outfile`: a file name that will contain the output of the Fidex algorithm\n",
    "- `--nb_attributes`: the number of attributes present in the dataset\n",
    "- `--nb_classes`: the number of classes present in the dataset\n",
    "\n",
    "There is also one optional argument that we are going to use:\n",
    "- `--root_folder`: path defining the root directory where every other path specified in other arguments begins\n",
    "\n",
    "All steps done until now will allow us to run the Fidex program. To see what happens, we launch it with just one sample. Therefore, we have saved beforehand the test data sample with its class and predictions in the file `data/CracksDataset/testDataSample.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "add3ff77-f54f-4288-b7fa-3c780a617653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ber generation, 0=random. Anything else than 0 is an arbitrary seed that can be reused to obtain the same randomly generated sequence and therefore getting same results (default: 0)\n",
      "\n",
      "----------------------------\n",
      "\n",
      "Execution example :\n",
      "\n",
      "fidex.fidex(\"--train_data_file datanormTrain.txt --train_pred_file predTrain.out --train_class_file dataclass2Train.txt --test_data_file testSampleDataCombine.txt --nb_attributes 16 --nb_classes 2 --weights_file weights.wts --rules_outfile rules.rls --stats_file stats.txt --root_folder dimlp/datafiles\")\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Parameters list:\n",
      " - train_data_file                                                     data/CracksDataset/trainData.txt\n",
      " - train_pred_file                                                     data/CracksDataset/predTrain.out\n",
      " - train_class_file                                                   data/CracksDataset/trainClass.txt\n",
      " - test_data_file                                                 data/CracksDataset/testDataSample.txt\n",
      " - rules_outfile                                                            data/CracksDataset/rule.rls\n",
      " - root_folder                                                                      data/CracksDataset/\n",
      " - weights_file                                                          data/CracksDataset/weights.wts\n",
      " - nb_attributes                                                                                   4096\n",
      " - nb_classes                                                                                         2\n",
      " - nb_quant_levels                                                                                   50\n",
      " - max_iterations                                                                                    10\n",
      " - min_covering                                                                                       2\n",
      " - max_failed_attempts                                                                               30\n",
      " - positive_class_index                                                                              -1\n",
      " - seed                                                                                               0\n",
      " - decision_threshold                                                                         -1.000000\n",
      " - hi_knot                                                                                     5.000000\n",
      " - dropout_hyp                                                                                 0.000000\n",
      " - dropout_dim                                                                                 0.000000\n",
      " - min_fidelity                                                                                1.000000\n",
      " - lowest_min_fidelity                                                                         0.750000\n",
      " - covering_strategy                                                                                  1\n",
      "End of Parameters list.\n",
      "\n",
      "Import files...\n",
      "\n",
      "Import time = 11.9531 sec\n",
      "Files imported\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "Creation of hyperspace...\n",
      "\n",
      "Parameters of hyperLocus :\n",
      "\n",
      "- Number of stairs 50\n",
      "- Interval : [-5,5]\n",
      "\n",
      "Import weight file...\n",
      "Weight file imported\n",
      "\n",
      "computation of hyperLocus\n",
      "HyperLocus computed\n",
      "\n",
      "Hyperspace created\n",
      "\n",
      "Searching for discriminating hyperplans...\n",
      "Initial fidelity : 0.498437\n",
      "Final fidelity : 1\n",
      "Discriminating hyperplans generated.\n",
      "\n",
      "\n",
      "Extracted rule :\n",
      "X3510<51 -> class 1\n",
      "   Train Covering size : 66\n",
      "   Train Fidelity : 1\n",
      "   Train Accuracy : 1\n",
      "   Train Confidence : 0.999392\n",
      "\n",
      "Result found after 1 iterations.\n",
      "-------------------------------------------------\n",
      "\n",
      "Time without data import = 1.54688 sec\n",
      "\n",
      "Full execution time = 13.5 sec\n"
     ]
    }
   ],
   "source": [
    "args = f\"\"\"\n",
    "        --root_folder {rootDir}\n",
    "        --train_data_file {trainDataFile}\n",
    "        --train_class_file {trainClassFile}\n",
    "        --train_pred_file {trainPredFile}\n",
    "        --test_data_file testDataSample.txt\n",
    "        --weights_file {weightsFile}\n",
    "        --rules_outfile rule.rls\n",
    "        --nb_attributes {nattributes}\n",
    "        --nb_classes {nclasses}\n",
    "        \"\"\"\n",
    "\n",
    "status = fidex(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dcf258-9284-4a41-817b-9158f321a2df",
   "metadata": {},
   "source": [
    "The output of the algorithm shows us, in the terminal, a walkthrough of the process. At the end of it, you can observe the generated rule. Let's have a closer look at it by extracting the freshly written rule file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d3c9efb-c7c9-473a-889d-328dd34d9f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No decision threshold is used.\n",
      "\n",
      "Rule for sample 0 :\n",
      "\n",
      "X3510<51 -> class 1\n",
      "   Train Covering size : 66\n",
      "   Train Fidelity : 1\n",
      "   Train Accuracy : 1\n",
      "   Train Confidence : 0.999392\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "previewFile(\"data/CracksDataset/rule.rls\", 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29899421-9b4b-4c0d-b6e4-7cc877bdf16e",
   "metadata": {},
   "source": [
    "The output displays a preview of a rule generated by Fidex. Each rule includes various properties:\n",
    "- The index of the sample from which the rule has been generated\n",
    "- The rule itself, composed of a single or list of antecedents and the prediction\n",
    "- The number of samples, in the training dataset, covered by the rule\n",
    "- The fidelity of the rule according to the model's predictions\n",
    "- The accuracy of the rule\n",
    "- The confidence of the rule with its choices, concerning the prediction values\n",
    "\n",
    "In the antecedents, the Xi terms represent the ith pixel of the image (or ith attribute).\n",
    "\n",
    "These rules provide insights into the model's predictions for each sample, helping to explain its decision-making process.\n",
    "\n",
    "It is possible to run Fidex with all test samples to generate an explaining rule for each sample. However, we will skip this step because it would take too much time due to the size of the dataset.\n",
    "\n",
    "In the next chapter, we will move on to global ruleSet generation using FidexGloRules. This will help us understand the overall behavior of the model by generating a set of global rules.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2c1cad-7802-47e5-bc09-aa28967913cc",
   "metadata": {},
   "source": [
    "# Global ruleSet generation - FidexGlo\n",
    "We have seen how to compute a rule that explains the decision of the model for a specific sample with the Fidex algorithm. But how could we get a general set of rules that characterizes the whole train dataset ? Using the [FidexGloRules](https://hes-xplain.github.io/documentation/algorithms/fidex/fidexglorules) algorithm, it is possible to achieve this.\n",
    "\n",
    "A global ruleset is a collection of rules that explains the model's decision for each sample present on the training portion of the dataset. Let's have a look at the fidexGloRules arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8286ec5a-306b-4048-84cb-7358c2647863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Warning! The files are located with respect to the root folder dimlpfidex.\n",
      "The arguments can be specified in the command or in a json configuration file with --json_config_file your_config_file.json.\n",
      "\n",
      "----------------------------\n",
      "\n",
      "Required parameters:\n",
      "\n",
      "--train_data_file <str>       Path to the file containing the train portion of the dataset\n",
      "--train_class_file <str>      Path to the file containing the train true classes of the dataset, not mandatory if classes are specified in train data file\n",
      "--train_pred_file <str>       Path to the file containing predictions on the train portion of the dataset\n",
      "--weights_file <str>          Path to the file containing the trained weights of the model (not mandatory if a rules file is given with --rules_file)\n",
      "--rules_file <str>            Path to the file containing the trained rules to be converted to hyperlocus (not mandatory if a weights file is given with --weights_file)\n",
      "--global_rules_outfile <str>  Path to the file where the output rule(s) will be stored. If a .json extension is given, rules are saved in JSON format\n",
      "--heuristic <int [1,3]>       Heuristic 1: optimal fidexGlo, 2: fast fidexGlo 3: very fast fidexGlo. (Faster algorithms are less efficient)\n",
      "--nb_attributes <int [1,inf[> Number of attributes in the dataset\n",
      "--nb_classes <int [2,inf[>    Number of classes in the dataset\n",
      "\n",
      "----------------------------\n",
      "\n",
      "Optional parameters: \n",
      "\n",
      "--json_config_file <str>      Path to the JSON file that configures all parameters. If used, this must be the sole argument and must specify the file's relative path\n",
      "--root_folder <str>           Path to the folder, based on main default folder dimlpfidex, containing all used files and where generated files will be saved. If a file name is specified with another option, its path will be relative to this root folder\n",
      "--attributes_file <str>       Path to the file containing the labels of attributes and classes\n",
      "--console_file <str>          Path to the file where the terminal output will be redirected. If not specified, all output will be shown on your terminal\n",
      "--max_iterations <int [1,inf[>\n",
      "                              Maximum iteration number, also the maximum possible number of attributes in a rule, it should be 25 when working with images (default: 10)\n",
      "--min_covering <int [1,inf[>  Minimum number of samples covered by the generated rules (default: 2)\n",
      "--covering_strategy <bool>    Whether to use the covering strategy : if no rule is found with min_covering, find best rule with best covering using dichotomic search. Decreases min_fidelity if needed (default: True)\n",
      "--max_failed_attempts <int [0,inf[>\n",
      "                              Maximum number of failed attempts to find a Fidex rule when the covering is 1 and the covering strategy is used (default: 30)\n",
      "--min_fidelity <float [0,1]>  Minimal rule fidelity accepted when generating a rule (default: 1.0)\n",
      "--lowest_min_fidelity <float [0,1]>\n",
      "                              Minimal min_fidelity to which we agree to go down during the covering_strategy (default: 0.75)\n",
      "--dropout_dim <float [0,1]>   Probability of dropping a dimension during rule extraction (default: 0.0)\n",
      "--dropout_hyp <float [0,1]>   Probability of dropping a hyperplane during rule extraction (default: 0.0)\n",
      "--decision_threshold <float [0,1]>\n",
      "                              The decision threshold used for predictions, you need to specify the index of the positive class if you want to use it\n",
      "--positive_class_index <int [0,nb_classes-1]>\n",
      "                              Index of the positive class for the usage of a decision threshold, index starts at 0\n",
      "--nb_quant_levels <int [3,inf[>\n",
      "                              Number of stairs in the staircase activation function (default: 50)\n",
      "--normalization_file <str>    Path to the file containing the mean and standard deviation of some attributes. Used to denormalize the rules if specified\n",
      "--mus <list<float ]-inf,inf[>>\n",
      "                              Mean or median of each attribute index to be denormalized in the rules\n",
      "--sigmas <list<float ]-inf,inf[>>\n",
      "                              Standard deviation of each attribute index to be denormalized in the rules\n",
      "--normalization_indices <list<int [0,nb_attributes-1]>>\n",
      "                              Attribute indices to be denormalized in the rules, only used when no normalization_file is given, index starts at 0 (default: [0,...,nb_attributes-1])\n",
      "--nb_threads <int [1,nb_cores]>\n",
      "                              Number of threads used for computing the algorithm, 1=sequential execution (default: 1)\n",
      "--seed <int [0,inf[>          Seed for random number generation, 0=random. Anything else than 0 is an arbitrary seed that can be reused to obtain the same randomly generated sequence and therefore getting same results (default: 0)\n",
      "\n",
      "----------------------------\n",
      "\n",
      "Execution example :\n",
      "\n",
      "fidex.fidexGloRules(\"--train_data_file datanormTrain.txt --train_pred_file predTrain.out --train_class_file dataclass2T"
     ]
    }
   ],
   "source": [
    "status = fidexGloRules(\"--help\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665be28c-c788-41c2-ae00-b7a7b3118c2b",
   "metadata": {},
   "source": [
    "Meanwhile, there are `required parameters` very similar to the `Fidex` algorithm, there are many optional arguments that you can use to customize the behavior of the algorithm. Let's have a look at some of them:\n",
    "\n",
    "- `--heuristic`: various ways to run the algorithm, these ways aim to increase execution speed. But also has a performance impact on results.\n",
    "- `--nb_threads`: number of threads used to compute the algorithm. Accelerate the process.\n",
    "- `--min_covering`: minimal number of samples a rule must cover\n",
    "- `--max_failed_attempts`: maximum failed attempts allowed when generating a rule\n",
    "- `--min_fidelity`: minimal fidelity allowed when generating a rule\n",
    "- `--max_iterations`: maximum number of iterations, also the maximum possible number of antecedents in a rule\n",
    "- `--nb_quant_levels`: number of stairs in the staircase activation function\n",
    "- `--dropout_dim`: probability of dropping a dimension when generating a rule\n",
    "- `--dropout_hyp`: probability of dropping a hyperplane when generating a rule\n",
    "- `--console_file`: a file where console outputs are redirected\n",
    "\n",
    "The process is very long and can last several days, so we already computed it beforehand. The ruleset is available in the file `data/CracksDataset/globalRules.txt`.\n",
    "\n",
    "If you want to launch it, and you have several processors available, you should add the parameter nb_threads with the number of processors that you want to use, it can speed up the process a lot. If you want to accelerate the process even more, you can use some dropout, the algorithm will randomly skip some dimensions or some hyperplans. For example, you can put both dropouts to 0.9, to skip nine out of ten dimensions and hyperplanes, which should be a lot faster. You just need to uncomment the next lines :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26e0d897-ab14-4e8d-a0f9-da7fe35bfb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rain.txt --weights_file weights.wts --nb_attributes 16 --nb_classes 2 --heuristic 1 --global_rules_outfile globalRules.rls --root_folder dimlp/datafiles\")\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "args = f\"\"\"\n",
    "        --root_folder {rootDir} \n",
    "        --nb_threads 4\n",
    "        --train_data_file {trainDataFile}\n",
    "        --train_class_file {trainClassFile}\n",
    "        --train_pred_file {trainPredFile}\n",
    "        --weights_file {weightsFile}\n",
    "        --nb_attributes {nattributes}\n",
    "        --nb_classes {nclasses}\n",
    "        --heuristic 1\n",
    "        --global_rules_outfile {globalRulesFile}\n",
    "        --max_iterations 25\n",
    "        --dropout_hyp 0.9\n",
    "        --dropout_dim 0.9\n",
    "        --nb_quant_levels 100\n",
    "        --console_file fidexGloRulesResult.txt\n",
    "        \"\"\"\n",
    "#status = fidexGloRules(args)\n",
    "#if (status == 0):\n",
    "#    print(\"fidexGloRules done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96311b9b-ef90-46dd-9f0a-b497cdce3372",
   "metadata": {},
   "source": [
    "The algorithm generated a file that we're going to partially observe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b15d8cd1-dea9-4db6-b815-83d85e1cd047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rules : 423, mean sample covering number per rule : 83.775414, mean number of antecedents per rule : 7.621749\n",
      "No decision threshold is used.\n",
      "\n",
      "Rule 1: X2528<102 X465>=102 -> class 1\n",
      "   Train Covering size : 984\n",
      "   Train Fidelity : 1\n",
      "   Train Accuracy : 1\n",
      "   Train Confidence : 0.999699\n",
      "\n",
      "Rule 2: X2468<117.3 X1231>=150.45 -> class 1\n",
      "   Train Covering size : 898\n",
      "   Train Fidelity : 1\n",
      "   Train Accuracy : 1\n",
      "   Train Confidence : 0.999741\n",
      "\n",
      "Rule 3: X2723<104.55 X1804>=117.3 -> class 1\n",
      "   Train Covering size : 733\n",
      "   Train Fidelity : 1\n",
      "   Train Accuracy : 1\n",
      "   Train Confidence : 0.999641\n",
      "\n",
      "Rule 4: X2470<102 X1375>=140.25 -> class 1\n",
      "   Train Covering size : 708\n",
      "   Train Fidelity : 1\n",
      "   Train Accuracy : 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "previewFile(\"data/CracksDataset/globalRules.rls\", 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e7b780-5fc6-4d0a-a82e-1b86ed6853d4",
   "metadata": {},
   "source": [
    "> *The algorithm result is subject to randomness as it uses random processes to compute. Results may differ between executions.*\n",
    "\n",
    "You can observe the rules are ordered by their covering size. The first rule is the one that best describes the training portion of the dataset. The algorithm generated about 400 rules explaining the whole train dataset. You can see at the top of the file the number of rules, the mean covering number per rule, and the mean number of antecedents. Here is an example of a rule that you may obtain:<be>\n",
    "\n",
    "```md\n",
    "Rule 1: X2528<102 X465>=102 -> class 1\n",
    "   Train Covering size : 984\n",
    "   Train Fidelity : 1\n",
    "   Train Accuracy : 1\n",
    "   Train Confidence : 0.999699\n",
    "```\n",
    "\n",
    "This rule is the first rule, which means that it's the rule with the maximum covering. Here, 984 train samples verify this rule. It is **100% fidel** with the model and is **100% accurate**.\n",
    "This rule says that if the 2528th pixel of the image is smaller than 102 and the 465th is greater than 102, then this image contains cracks. And this rule has **99.9% of confidence**.<br>\n",
    "\n",
    "To get statistics on the test portion of the dataset, let's execute the [fidexGloStats](https://hes-xplain.github.io/documentation/algorithms/fidex/fidexglostats) algorithm. Beginning with an overview of the arguments of the program: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d6aaa70-4ffa-4205-99f8-840d67b260cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Warning! The files are located with respect to the root folder dimlpfidex.\n",
      "The arguments can be specified in the command or in a json configuration file with --json_config_file your_config_file.json.\n",
      "\n",
      "----------------------------\n",
      "\n",
      "Required parameters:\n",
      "\n",
      "--test_data_file <str>        Path to the file containing the test portion of the dataset\n",
      "--test_class_file <str>       Path to the file containing the test true classes of the dataset, not mandatory if classes are specified in test data file\n",
      "--test_pred_file <str>        Path to the file containing predictions on the test portion of the dataset\n",
      "--global_rules_file <str>     Path to the file containing the global rules obtained with fidexGloRules algorithm.\n",
      "--nb_attributes <int [1,inf[> Number of attributes in the dataset\n",
      "--nb_classes <int [2,inf[>    Number of classes in the dataset\n",
      "\n",
      "----------------------------\n",
      "\n",
      "Optional parameters: \n",
      "\n",
      "--json_config_file <str>      Path to the JSON file that configures all parameters. If used, this must be the sole argument and must specify the file's relative path\n",
      "--root_folder <str>           Path to the folder, based on main default folder dimlpfidex, containing all used files and where generated files will be saved. If a file name is specified with another option, its path will be relative to this root folder\n",
      "--global_rules_outfile <str>  Path to the file where the output global rules will be stored with stats on test set, if you want to compute those statistics.\n",
      "--attributes_file <str>       Path to the file containing the labels of attributes and classes> Mandatory if rules file contains attribute names, if not, do not add it\n",
      "--stats_file <str>            Path to the file where statistics of the global ruleset will be stored\n",
      "--console_file <str>          Path to the file where the terminal output will be redirected. If not specified, all output will be shown on your terminal\n",
      "--positive_class_index <int [0,nb_classes-1]>\n",
      "                              Index of the positive class to compute true/false positive/negative rates, index starts at 0. If it is specified in the rules file, it has to be the same value.\n",
      "\n",
      "----------------------------\n",
      "\n",
      "Execution example :\n",
      "\n",
      "fidex.fidexGloStats(\"--test_data_file datanormTest.txt --test_pred_file predTest.out --test_class_file dataclass2Test.txt --global_rules_file globalRules.rls --nb_attributes 16 --nb_classes 2 --stats_file stats.txt --root_folder dimlp/datafiles\")\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "status = fidexGloStats(\"--help\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b7ebed-01b9-4016-addb-d99839f7be74",
   "metadata": {},
   "source": [
    "As you can observe, the required arguments are pretty much the same as previous executions. The only one that differs is `--global_rules_file` which simply asks to input the `global rule file` to compute statistics. With the parameter `--global_rules_outfile` we can generate the statistics on rules which will modify the rules file. If you want to keep the first rule set unchanged, you should give another name.\n",
    "\n",
    "Let's try this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "375ef5c0-aca8-4e1a-a842-ed229d23e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters list:\n",
      " - test_data_file                                                       data/CracksDataset/testData.txt\n",
      " - test_pred_file                                                       data/CracksDataset/predTest.out\n",
      " - test_class_file                                                     data/CracksDataset/testClass.txt\n",
      " - global_rules_outfile                                 data/CracksDataset/globalRulesWithTestStats.rls\n",
      " - global_rules_file                                                 data/CracksDataset/globalRules.rls\n",
      " - root_folder                                                                      data/CracksDataset/\n",
      " - stats_file                                                      data/CracksDataset/fidexGloStats.txt\n",
      " - nb_attributes                                                                                   4096\n",
      " - nb_classes                                                                                         2\n",
      " - positive_class_index                                                                              -1\n",
      "End of Parameters list.\n",
      "\n",
      "Importing files...\n",
      "\n",
      "Data imported.\n",
      "\n",
      "Compute statistics...\n",
      "\n",
      "Global statistics of the rule set : \n",
      "Number of rules : 423, mean sample covering number per rule : 83.775414, mean number of antecedents per rule : 7.621749\n",
      "\n",
      "Statistics with a test set of 1600 samples :\n",
      "\n",
      "No decision threshold is used.\n",
      "No positive index class is used.\n",
      "The global rule fidelity rate is : 0.982500\n",
      "The global rule accuracy is : 0.983750\n",
      "The explainability rate (when we can find one or more rules, either correct ones or activated ones which all agree on the same class) is : 0.945625\n",
      "The default rule rate (when we can't find any rule activated for a sample) is : 0.054375\n",
      "The mean number of correct(fidel) activated rules per sample is : 5.243125\n",
      "The mean number of wrong(not fidel) activated rules per sample is : 0.270625\n",
      "The model test accuracy is : 0.996250\n",
      "The model test accuracy when rules and model agree is : 0.998728\n",
      "The model test accuracy when activated rules and model agree is : 0.999327\n",
      "\n",
      "Full execution time = 3.0625 sec\n"
     ]
    }
   ],
   "source": [
    "args = f\"\"\"\n",
    "        --root_folder {rootDir}\n",
    "        --test_data_file {testDataFile}\n",
    "        --test_class_file {testClassFile}\n",
    "        --test_pred_file {testPredFile}\n",
    "        --global_rules_file {globalRulesFile}\n",
    "        --nb_attributes {nattributes}\n",
    "        --nb_classes {nclasses}\n",
    "        --stats_file fidexGloStats.txt\n",
    "        --global_rules_outfile globalRulesWithTestStats.rls\n",
    "        \"\"\"\n",
    "\n",
    "status = fidexGloStats(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c523c382-b2ed-413e-84e7-c0d39c6d1bf5",
   "metadata": {},
   "source": [
    "The execution of the algorithm generated a file that we named `stats.txt` containing pretty much the same feedback as the program output.\n",
    "\n",
    "The output of the program shows various metrics, let's have a look at them individually:\n",
    "\n",
    "- `Global statistics`: Several values expressing general information about the ruleset.\n",
    "- `Decision threshold`: Value used to define a threshold where a class is considered as true. In this case, it's written that `no decision threshold is used`.\n",
    "- `Positive index class`: This value means which class is considered as the positive one. If no threshold is used, this cannot be used, like in this case.\n",
    "- `Global rule fidelity rate`: Expressing whether the ruleset accurately reflects the model's predictions.\n",
    "- `Global rule accuracy`: Proportion of correct predictions made by the ruleset.\n",
    "- `Explainability rate`: Proportion of the samples that could be explained by one or more rules.\n",
    "- `Default rule rate`: Proportion of samples that could not be explained by a rule offered by the ruleset.\n",
    "- `Mean number of correct activated rules`: Average number of correct rules activated per sample.\n",
    "- `Mean number of wrong activated rules`: Average number of incorrect rules activated per sample.\n",
    "- `Model test accuracy`: Accuracy of the model on the test dataset\n",
    "- `Model test accuracy when rules agree`: Accuracy of the model on test samples where the ruleset and model predictions agree.\n",
    "- `Model test accuracy when activated rules agree`: Accuracy when at least one activated rule agrees with the model's prediction.\n",
    "\n",
    "With this program, you can have a general overview of the quality of the ruleset.\n",
    "\n",
    "We have about **98% fidelity**, which is good, and a **rule accuracy(98.3%)** about 1% lower than the **model accuracy(99.6%)**. So the rules seem to classify a bit worse. We have more than a **94% explainability rate**, so only in 6% of cases do we need to compute Fidex to get a rule. Each sample can activate many rules. Here on average, a sample activates **5 correct rules** and **0.3 wrong rules**. A wrong rule is a rule with which the model doesn't agree. For example, if the rule says 0 and the model says 1. Something interesting is the `model test accuracy when rules and model agree`. You can see that, generally, the accuracy increases if we consider samples where rules and model agree, and increases even more if we take only the activated rules (when there are no activated rules, we choose the model prediction). That means that the rules confirm well the model decision, but when no rule is found, the model decision may as well be wrong. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5400680-5058-4f75-97f1-552b1cead6d8",
   "metadata": {},
   "source": [
    "Finally, in the `globalRulesWithTestStats` file, you can now see the statistics of rules on the test set. Here is the same rule seen before with the test statistics:<be>\n",
    "\n",
    "```md\n",
    "Rule 1: X2528<102 X465>=102 -> class 1\n",
    "   Train Covering size : 984 --- Test Covering size : 246\n",
    "   Train Fidelity : 1 --- Test Fidelity : 0.995935\n",
    "   Train Accuracy : 1 --- Test Accuracy : 0.995935\n",
    "   Train Confidence : 0.999699 --- Test Confidence : 0.994714\n",
    "```\n",
    "\n",
    "We see that the rule no longer always agrees with the model, only in 99.6% of cases, and the rule is not perfectly accurate (99.6%) on the test set. That means that the rule is very good in reality, with **99.6% of correct classification**.\n",
    "\n",
    "In the next chapter, we will get a comprehensive explanation for each sample using `FidexGlo` and with image generation. This will help us understand better the behavior of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72abe27f-3d11-4128-90ab-307e5e16d5a6",
   "metadata": {},
   "source": [
    "# Explanation and image generation\n",
    "\n",
    "Now let's get the explanations on a few test samples. We will use the [fidexGlo](https://hes-xplain.github.io/documentation/algorithms/fidex/fidexglo) algorithm with the files `testDataSamples`, `testClassSamples`, and `testPredSamples` containing 10 test samples. <br>\n",
    "Beginning with an overview of the arguments of the program: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4687bf09-3954-401f-a36b-37891a2fb355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Warning! The files are located with respect to the root folder dimlpfidex.\n",
      "The arguments can be specified in the command or in a JSON configuration file with --json_config_file your_config_file.json.\n",
      "\n",
      "----------------------------\n",
      "\n",
      "Required parameters:\n",
      "\n",
      "--test_data_file <str>        Path to the file containing test sample(s) data, prediction (if no --test_pred_file) and true classes if launching with fidex (--with_fidex and if no --test_class_file)\n",
      "--global_rules_file <str>     Path to the file containing the global rules obtained with fidexGloRules algorithm.\n",
      "--nb_attributes <int [1,inf[> Number of attributes in the dataset\n",
      "--nb_classes <int [2,inf[>    Number of classes in the dataset\n",
      "\n",
      "----------------------------\n",
      "\n",
      "Optional parameters: \n",
      "\n",
      "--json_config_file <str>      Path to the JSON file that configures all parameters. If used, this must be the sole argument and must specify the file's relative path\n",
      "--root_folder <str>           Path to the folder, based on main default folder dimlpfidex, containing all used files and where generated files will be saved. If a file name is specified with another option, its path will be relative to this root folder\n",
      "--test_pred_file <str>        Path to the file containing predictions on the test portion of the dataset. If given, --test_data_file needs to have only the test data\n",
      "--explanation_file <str>      Path to the file where explanation(s) will be stored\n",
      "--attributes_file <str>       Path to the file containing the labels of attributes and classes. Mandatory if rules file contains attribute names; if not, do not add it\n",
      "--console_file <str>          Path to the file where the terminal output will be redirected. If not specified, all output will be shown on your terminal\n",
      "--with_minimal_version <bool> Whether to use the minimal version, which only gets correct activated rules and if with_fidex, launches Fidex when no such rule is found (default: False)\n",
      "--with_fidex <bool>           Whether to use the Fidex algorithm if no rule is found in global rules (default: False)\n",
      "\n",
      "----------------------------\n",
      "\n",
      "If using fidex :\n",
      "\n",
      "Required :\n",
      "\n",
      "--train_data_file <str>       Path to the file containing the train portion of the dataset\n",
      "--train_class_file <str>      Path to the file containing the train true classes of the dataset, not mandatory if classes are specified in train data file\n",
      "--train_pred_file <str>       Path to the file containing predictions on the train portion of the dataset\n",
      "--weights_file <str>          Path to the file containing the trained weights of the model (not mandatory if a rules file is given with --rules_file)\n",
      "--rules_file <str>            Path to the file containing the trained rules to be converted to hyperlocus (not mandatory if a weights file is given with --weights_file)\n",
      "\n",
      "Optional :\n",
      "\n",
      "--test_class_file <str>       Path to the file containing the test true classes of the dataset. Classes can be specified in test data file\n",
      "--max_iterations <int [1,inf[>\n",
      "                              Maximum iteration number, also the maximum possible number of attributes in a rule, it should be 25 when working with images (default: 10)\n",
      "--min_covering <int [1,inf[>  Minimum number of samples covered by the generated rules (default: 2)\n",
      "--covering_strategy <bool>    Whether to use the covering strategy : if no rule is found with min_covering, find best rule with best covering using dichotomic search. Decreases min_fidelity if needed (default: True)\n",
      "--max_failed_attempts <int [0,inf[>\n",
      "                              Maximum number of failed attempts to find a Fidex rule when the covering is 1 and the covering strategy is used (default: 30)\n",
      "--min_fidelity <float [0,1]>  Minimal rule fidelity accepted when generating a rule (default: 1.0)\n",
      "--lowest_min_fidelity <float [0,1]>\n",
      "                              Minimal min_fidelity to which we agree to go down during the covering_strategy (default: 0.75)\n",
      "--nb_fidex_rules <int [1,inf[>\n",
      "                              Number of Fidex rules to compute per sample when launching the Fidex algorithm (default: 1)\n",
      "--dropout_dim <float [0,1]>   Probability of dropping a dimension during rule extraction (default: 0.0)\n",
      "--dropout_hyp <float [0,1]>   Probability of dropping a hyperplane during rule extraction (default: 0.0)\n",
      "--nb_quant_levels <int [3,inf[>\n",
      "                              Number of stairs in the staircase activation function (default: 50)\n",
      "--normalization_file <str>    Path to the file containing the mean and standard deviation of some attributes. Used to denormalize the rules if specified\n",
      "--mus <list<float ]-inf,inf[>>\n",
      "                              Mean or median of each attribute index to be denormalized in the rules\n",
      "--sigmas <list<float ]-inf,inf[>>\n",
      "                              Standard deviation of each attribute index to be denormalized in the rules\n",
      "--normalization_indices <list<int [0,nb_attributes-1]>>\n",
      "                              Attribute indices to be denormalized in the rules, only used when no normalization_file is given, index starts at 0 (default: [0,...,nb_attributes-1])\n",
      "--seed <int [0,inf[>          Seed for random number generation, 0=random. Anything else than 0 is an arbitrary seed that can be reused to obtain the same randomly generated sequence and therefore getting same results (default: 0)\n",
      "\n",
      "----------------------------\n",
      "\n",
      "Execution example :\n",
      "\n",
      "fidex.fidexGlo(\"--test_data_file datanormTest.txt --test_pred_file predTest.out --global_rules_file globalRules.rls --nb_attributes 16 --nb_classes 2 --explanation_file explanation.txt --root_folder dimlp/datafiles --with_fidex true --train_data_file datanormTrain.txt --train_pred_file predTrain.out --train_class_file dataclass2Train.txt --test_class_file dataclass2Test.txt --weights_file weights.wts\")\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "status = fidexGlo(\"--help\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc831c7-8039-4472-aa77-40c197a2cbe1",
   "metadata": {},
   "source": [
    "As you can observe, the required arguments are pretty much the same as previous executions. The more important argument that differs is `--with_fidex` telling whether we want to execute `Fidex` when no rule is activated in the global rule set for a given sample.\n",
    "\n",
    "We execute the algorithm like this :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd579681-492d-4900-84ae-69aeef690a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fidexGlo done\n"
     ]
    }
   ],
   "source": [
    "args = f\"\"\"\n",
    "        --root_folder {rootDir}\n",
    "        --test_data_file testDataSamples.txt\n",
    "        --test_class_file testClassSamples.txt\n",
    "        --test_pred_file testPredSamples.txt\n",
    "        --global_rules_file {globalRulesFile}\n",
    "        --nb_attributes {nattributes}\n",
    "        --nb_classes {nclasses}\n",
    "        --explanation_file explanations.txt\n",
    "        --console_file fidexGloResults.txt\n",
    "        --with_fidex true\n",
    "        --train_data_file {trainDataFile}\n",
    "        --train_pred_file {trainPredFile}\n",
    "        --train_class_file {trainClassFile}\n",
    "        --weights_file {weightsFile}\n",
    "        \"\"\"\n",
    "\n",
    "status = fidexGlo(args)\n",
    "if (status == 0):\n",
    "   print(\"fidexGlo done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccf58f4-b70b-4586-ad3c-345d9a5d0823",
   "metadata": {},
   "source": [
    "The explanations for each test sample can then be found in the file `explanations.txt`. Let's have a look inside the generated file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3922a0a7-de5b-4409-bac9-ed3fe84fbf3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global statistics of the rule set : \n",
      "Number of rules : 423, mean sample covering number per rule : 83.775414, mean number of antecedents per rule : 7.621749\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Explanation for sample 0 :\n",
      "\n",
      "The model predicts class 1 with probability 0.997448\n",
      "\n",
      "We didn't find any rule with the same prediction as the model (class 1), but we found 9 rules with class 0 :\n",
      "\n",
      "R1: X2528>=140.25 X1961>=163.2 X2396>=153 X1687>=168.3 X3487>=165.75 X2910>=153 X1637>=145.35 X2404>=155.55 X2073>=142.8 X748>=142.8 X1031>=163.2 X2261>=155.55 X1032<173.4 X1543>=127.5 -> class 0\n",
      "   Train Covering size : 152\n",
      "   Train Fidelity : 1\n",
      "   Train Accuracy : 0.993421\n",
      "   Train Confidence : 0.999216\n",
      "\n",
      "R2: X2329>=178.5 X2340>=178.5 X1754>=183.6 X1700>=173.4 X2592>=158.1 X182<186.15 X335<188.7 -> class 0\n",
      "   Train Covering size : 135\n",
      "   Train Fidelity : 1\n",
      "   Train Accuracy : 1\n",
      "   Train Confidence : 0.999402\n",
      "\n",
      "R3: X2405>=173.4 X2266>=158.1 X1876>=178.5 X1637>=178.5 X2657>=173.4 X2327>=181.05 X3095>=175.95 X2931<183.6 X1353<193.8 -> class 0\n",
      "   Train Covering size : 105\n",
      "   Train Fidelity : 1\n",
      "   Train Accuracy : 1\n",
      "   Train Confidence : 0.999601\n",
      "\n",
      "R4: X2657>=170.85 X2264>=170.85 X1766>=173.4 X2403>=158.1 X1815>=170.85 X2967>=175.95 X1569>=173.4 X2276>=168.3 X1639<183.6 X3028<181.05 -> class 0\n",
      "   Train Covering size : 83\n",
      "   Train Fidelity : 1\n",
      "   Train Accuracy : 1\n",
      "   Train Confidence : 0.999705\n",
      "\n",
      "R5: X2656>=153 X2470>=153 X1812>=155.55 X1701>=147.9 X2199>=150.45 X2520>=150.45 X2717>=147.9 X3621>=150.45 X561>=137.7 X1222>=145.35 X2403>=150.45 X3646<153 X2855>=76.5 -> class 0\n",
      "   Train Covering size : 42\n",
      "   Train Fidelity : 1\n",
      "   Train Accuracy : 1\n",
      "   Train Confidence : 0.998985\n",
      "\n",
      "R6: X2402>=168.3 X2328>=170.85 X1765>=155.55 X1943>=160.65 X2534>=165.75 X2974>=158.1 X1820>=168.3 X565>=178.5 X450<183.6 X3250<173.4 -> class 0\n",
      "   Train Covering size : 36\n",
      "   Train Fidelity : 1\n",
      "   Train Accuracy : 1\n",
      "   Train Confidence : 0.999461\n",
      "\n",
      "R7: X2525>=160.65 X2212>=175.95 X1120>=178.5 X2466>=168.3 X1876>=150.45 X1700>=165.75 X3096>=165.75 X2704>=175.95 X1976<168.3 X3232>=89.25 -> class 0\n",
      "   Train Covering size : 30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "previewFile(\"data/CracksDataset/explanations.txt\", 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eebb8f-e27b-4428-9f28-cc24066f7b37",
   "metadata": {},
   "source": [
    "Many rules are activated for each sample and a global rule is found for each of those 10 samples, so `Fidex` was not called.\n",
    "\n",
    "Now, we parse this explanation file to get the first explanation rule of each sample. We generate the image corresponding to the sample and add colored pixels where the rule is activated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c423f7c5-ae3d-4c15-9a3a-2563ec2ec656",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "test_data = \"data/CracksDataset/testDataSamples.txt\"\n",
    "with open(test_data, \"r\") as my_file:\n",
    "    for line in my_file:\n",
    "        images.append(line.strip().split(\" \"))\n",
    "\n",
    "explanation_file = \"data/CracksDataset/explanations.txt\"\n",
    "pattern = r'X(\\d+)\\s*([<>]=?)\\s*([\\d.]+)' # Regular expression pattern to match antecedents\n",
    "rules = []\n",
    "with open(explanation_file, \"r\") as my_file:\n",
    "    for line in my_file:\n",
    "        if line.startswith(\"R1: \"):\n",
    "            rules.append(line.strip())\n",
    "        if line.startswith(\"Local rule\"):\n",
    "            # Search next non empty line\n",
    "            next_line = next(my_file, '').strip()\n",
    "            while not next_line:\n",
    "                next_line = next(my_file, '').strip()\n",
    "            rules.append(next_line)\n",
    "    my_file.close()\n",
    "\n",
    "# Find all matches in the input string\n",
    "for id_sample in range(len(rules)):\n",
    "    antecedents = []\n",
    "    matches = re.findall(pattern, rules[id_sample])\n",
    "\n",
    "    # Process each match and store in antecedents\n",
    "    for match in matches:\n",
    "        attribute, inequality, value = match\n",
    "        antecedent = {\n",
    "            \"attribute\": int(attribute),\n",
    "            \"inequality\": inequality,\n",
    "            \"value\": float(value)\n",
    "        }\n",
    "        antecedents.append(antecedent)\n",
    "\n",
    "    colorimage = [[v,v,v] for v in images[id_sample]]\n",
    "    for antecedent in antecedents:\n",
    "                if antecedent[\"inequality\"] == \"<\":\n",
    "                    colorimage[antecedent[\"attribute\"]]=[255,0,0]\n",
    "                else:\n",
    "                    colorimage[antecedent[\"attribute\"]]=[0,255,0]\n",
    "\n",
    "    colorimage_array = np.array(colorimage).reshape(64, 64, 3)\n",
    "    colorimage = Image.fromarray(colorimage_array.astype('uint8'))\n",
    "    image_path = 'data/CracksDataset/images/img_'+ str(id_sample) + '_out.png'\n",
    "    colorimage.save(image_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f912a26f-6d11-46b6-bd51-9f52f6f36b95",
   "metadata": {},
   "source": [
    "You can observe the 10 images in the `image` folder. Let's take a look at the third one:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48b2ed2-2c5b-4172-903a-0846f003f836",
   "metadata": {},
   "source": [
    "<center><img src=\"data/CracksDataset/images/sample2.png\" width=\"20%\" /></center>\n",
    "<center><i>Third sample</i></center>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780a4c87-064b-4ed7-bcc9-cfa08e147d03",
   "metadata": {},
   "source": [
    "The red dots indicate pixels where the rule requires the value to be below a certain threshold, while the green dots represent pixels where the value must be above a threshold. You can observe which pixels the model uses to decide if a crack is present. When a crack is present, there is generally a red dot in the crack and a green one outside. Otherwise, there are more green dots everywhere to check if a crack is present. See this sample per example :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ad9cd2-c2d8-4973-a7d6-a98b1e297f79",
   "metadata": {},
   "source": [
    "<center><img src=\"data/CracksDataset/images/sample1.png\" width=\"20%\" /></center>\n",
    "<center><i>Second sample</i></center>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3988ad46-3394-4e29-949b-fe000053f42f",
   "metadata": {},
   "source": [
    "By generating local and global rules using the Fidex algorithms, we have a clearer view of how our model makes predictions. These rules help us understand the model's decisions, making it more transparent. Let's wrap up our findings and discuss the importance of explainable AI in the final chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7f3208-c538-40eb-8c76-31e75d687198",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook, we explored explainable AI using CNNs and the Fidex family of algorithms. We looked at our dataset, trained a CNN model, and examined the generated rules. We used `Fidex` to create a local rule for a given sample explanation and `FidexGloRules` to generate a global ruleset for the entire training dataset. Then, we evaluated the ruleset with `FidexGloStats`, providing insights into the model's accuracy, fidelity, and explainability. Finally, we generated some explications with `FidexGlo` and observed the rule directly on the image.\n",
    "\n",
    "This process demonstrated how explainable AI techniques can clarify complex models, making them more transparent and trustworthy. By understanding our model's decision-making process, we can ensure better, more reliable outcomes in various applications. Using CNNs with Fidex offers a balanced approach to building interpretable and effective AI models and it is possible to obtain explanations on any image dataset you want.\n",
    "\n",
    "To go further, you can explore any image dataset you want and get explanations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605dcb45-4862-4268-bef2-068cf96de3d9",
   "metadata": {},
   "source": [
    "# Références\n",
    "\n",
    "TODO : Article DimlpBT de Guido\n",
    "Article Fidex de Guido (à venir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e18eda-640c-4393-97ef-a3b5b7f682e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
